#Data Structures 

1> We are using the following struct for representing actions:

    typedef struct actionStruct {
        int priority_type;          // these are case dependent
        int priority_val;           // used for ordering per-case basis
        int user_id;
        int action_id;
        int action_type;
        unsigned int time_stamp;

        // creating a priority queue
        bool operator<(const actionStruct& rhs) const {
            return (priority_type) ? (time_stamp < rhs.time_stamp) : (priority_val < rhs.priority_val);
        }
    } Action;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

NOTE1>  `priority_type`, `priority_val` is dynamically added to an action at the time
        its being added to the feed-queue of a node, to order the feed-queue.
NOTE2>  The `operator<` is used for ordering in case of `priority_queue` (which is used
        by readPost)
NOTE3>  Here the `user_id` represents the node id which performed the action. 
        `action_id` is a counter for the global action count.
        For the `time_stamp` we are using Unix timestamp.

2> For representing the node we are using the following structure:

    typedef struct node {
        int priority; // 0->priority, 1->chronological
        int degree;
        int action_id[3]; 	
        queue<Action> WallQueue;
        priority_queue<Action> FeedQueue;
    } Node;

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

NOTE1>
    In this struct, we have a field for `priority`.

    `priority`: 0 -> then we consider the priority order
    `priority`: 1 -> we consider the chronological order
    The field degree stores the number of neighbors of the node.

NOTE2>
    Then we have an array action_id with three rows. 

    Action_id[0] : stores the counter for the action “post” 
    Action_id[1] : stores the counter for the action “comment” 
    Action_id[2] : stores the counter for the action “like” 

NOTE3>
    Then we have 2 queues to store the action details.
    `wallQueue` stores the action done by the node itself.
    `feedQueue` stores the actions done by the nodes' neighbors.

3> This global wall queue stores all the actions generated by the userSimulator thread.

    queue <Action> GlbWallQueue;

NOTE> This queue is shared by the userSimulator as well as the 25 pushUpdate threads 

#Usage of Locks/Deadlock Avoidance

1> We use `pthread_cond` variables for monitoring of locked variables, as well as
    thread activation from other threads.

2> 1 mutex `lock_wallq`-> Synchronizing the access of `GlbWallQueue`: 25 `pushUpdate`, 1 `userSimulator`
    2a> We use a mutex lock `lock_wallq` to lock the global wall queue.
            > `lock_wallq` is locked by `userSimulator`, and then  unlocked.
            > `lock_wallq` is also handled by `pushUpdate` to lock the queue
            > Hence there is no race-condition possible over the `GlbWallQueue`
        We use `cv_empty`, a `pthread_cond` variable for managing thread control
            > The variable wakes up the `pushUpdate` threads from the `userSimulator`
            > The variable is also used to broadcast from oner `pushUpdate` thread to another
            > In `pushUpdate` if the queue-lock `lock_wallq` is acquired but the 
                queue is empty, the thread waits (`pthread_cond_wait`), and when some
                other thread has acquired the mutex, and has popped the queue, it
                checks if more elements are present, and wakes up others accordingly
            > Hence threads are managed without deadlocks.


3> 10 mutexes `feed_lock` -> Synchronizing the access of `node.FeedQueue`, and (global) `feed_queue[10]`: 25 `pushUpdate`, 1 `userSimulator`
    NOTE> We have divided all the nodes on the basis of `node_id % 10` statically among
            the 10 `readPost` threads, hence 10 mutexes `feed_lock[10]` exist for synchronization.
    
    3a> We use the respective mutex lock acquired from the `user_id`(by % 10), to lock the
        respective node.FeedQueue, as well as the `feed_queue`. Similarly, `readPost`
        acquires the lock when its popping the posts.

Detailed Explanation:
    We are using the pthread_condition variables for monitoring the queue. The pool of 25 pushUpdate threads wait on the condition that the queue is empty.
    When a thread checks that the queue is not empty, it broadcasts the signal to all the threads waiting on this condition. So, a thread comes and locks the queue, then it checks if the queue is empty.
    If there are actions in the queue, it deques an action and checks if the queue is not empty, then it sends a broadcast to all the waiting threads.
    After that, it unlocks the lock. After unlocking it, the thread continues its work to push the dequeued action to the feed of the neighboring nodes.

#Concurrency Explanation

1> (25x) `pushUpdate`: the `lock_wallq` is released as soon as the `GlbWallQueue` is popped, and hence multiple
                of the 25 `pushUpdate` threads can concurrently execute the code to push to the node.FeedQueue.

2> (10x) `readPost`: The nodes are divided into 10 different classes, and for each class, one thread is statically assigned.

    [IMPORTANT]: Assuming the scale of the system, we have kept one mutex per node-group, and not one per node, given
                    the overhead of creation, as well as the memory requirement.

                **Similarly the other approach of having node level mutex, as well as all-threads-to-all-nodes mapping
                    leads to contention overhead between the `readPost` threads themselves

    [IMPORTANT]: At any given point the 10 threads will execute in parallel, with no contention, and blocking
                    among each other. The threads will continue to execute as long as any node in the node-group mapped
                    to the thread has `Action` remaining in the `FeedQueue`. When there's none, it `pthread_cond_wait`s.

NOTE> The same can be seen by the logfile messages.